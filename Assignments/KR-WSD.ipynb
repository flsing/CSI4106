{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"pull-right\"><img src=KEY-logo.png></div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge representation and similarity \n",
    "### Grounding (Word-Sense Disambiguation) to WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSI4106 Artificial Intelligence  \n",
    "Fall 2018  \n",
    "Caroline BarriÃ¨re\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, first, you will explore Wordnet, a lexical semantic network, in which knowledge is organized by interrelated synsets (groups of synonyms).  Second, you will attempt Word-Sense Disambiguation (WSD), using simple Lesk-like algorithm which compares BOWs (bag-of-words).  \n",
    "\n",
    "This notebook uses the same package NLTK as we used in the last notebook. We will also reuse some knowledge from the previous notebook (tokenization, lemmatization, POS tagging), so make sure to do the NLP Pipeline notebook before this one.\n",
    "\n",
    "*As you now have more experience, this notebook requires that you write more code by yourself than the previous ones.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***HOMEWORK***:  \n",
    "Go through the notebook by running each cell, one at a time. Look for (**TO DO**) for the tasks that you need to perform.  \n",
    "Make sure you *sign* (type your name) the notebook at the end. Once you're done, submit your notebook.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import nltk, and wordnet\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Exploring Wordnet**  \n",
    "\n",
    "Let's first explore a bit the wordnet interface within nltk.  \n",
    "You can also look a the [WordNet interface description](http://www.nltk.org/howto/wordnet.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('paper.n.01'), Synset('composition.n.08'), Synset('newspaper.n.01'), Synset('paper.n.04'), Synset('paper.n.05'), Synset('newspaper.n.02'), Synset('newspaper.n.03'), Synset('paper.v.01'), Synset('wallpaper.v.01')]\n"
     ]
    }
   ],
   "source": [
    "# a synset is a concept associated with a set of synonyms\n",
    "\n",
    "paperSenses = wordnet.synsets('paper')\n",
    "print(paperSenses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that there are 9 senses of paper, 7 nouns and 2 verbs.  The word displayed is the most representative word for each sense.  \n",
    "\n",
    "You can try other words.  I recommend that you also perform the same search [online](http://wordnetweb.princeton.edu/perl/webwn) to better understand the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the basic information in each synset.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a function to print the basic information\n",
    "\n",
    "def printBasicSynsetInfo(d):\n",
    "    print(\"SynLemmas\")\n",
    "    print(d.lemmas())\n",
    "    print(\"Synonyms\")\n",
    "    synonyms = [l.name() for l in d.lemmas()]\n",
    "    print(synonyms)\n",
    "    print(\"Definition\")\n",
    "    print(d.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sense 0]\n",
      "SynLemmas\n",
      "[Lemma('paper.n.01.paper')]\n",
      "Synonyms\n",
      "['paper']\n",
      "Definition\n",
      "a material made of cellulose pulp derived mainly from wood or rags or certain grasses\n",
      "\n",
      "[Sense 1]\n",
      "SynLemmas\n",
      "[Lemma('composition.n.08.composition'), Lemma('composition.n.08.paper'), Lemma('composition.n.08.report'), Lemma('composition.n.08.theme')]\n",
      "Synonyms\n",
      "['composition', 'paper', 'report', 'theme']\n",
      "Definition\n",
      "an essay (especially one written as an assignment)\n",
      "\n",
      "[Sense 2]\n",
      "SynLemmas\n",
      "[Lemma('newspaper.n.01.newspaper'), Lemma('newspaper.n.01.paper')]\n",
      "Synonyms\n",
      "['newspaper', 'paper']\n",
      "Definition\n",
      "a daily or weekly publication on folded sheets; contains news and articles and advertisements\n",
      "\n",
      "[Sense 3]\n",
      "SynLemmas\n",
      "[Lemma('paper.n.04.paper')]\n",
      "Synonyms\n",
      "['paper']\n",
      "Definition\n",
      "a medium for written communication\n",
      "\n",
      "[Sense 4]\n",
      "SynLemmas\n",
      "[Lemma('paper.n.05.paper')]\n",
      "Synonyms\n",
      "['paper']\n",
      "Definition\n",
      "a scholarly article describing the results of observations or stating hypotheses\n",
      "\n",
      "[Sense 5]\n",
      "SynLemmas\n",
      "[Lemma('newspaper.n.02.newspaper'), Lemma('newspaper.n.02.paper'), Lemma('newspaper.n.02.newspaper_publisher')]\n",
      "Synonyms\n",
      "['newspaper', 'paper', 'newspaper_publisher']\n",
      "Definition\n",
      "a business firm that publishes newspapers\n",
      "\n",
      "[Sense 6]\n",
      "SynLemmas\n",
      "[Lemma('newspaper.n.03.newspaper'), Lemma('newspaper.n.03.paper')]\n",
      "Synonyms\n",
      "['newspaper', 'paper']\n",
      "Definition\n",
      "the physical object that is the product of a newspaper publisher\n",
      "\n",
      "[Sense 7]\n",
      "SynLemmas\n",
      "[Lemma('paper.v.01.paper')]\n",
      "Synonyms\n",
      "['paper']\n",
      "Definition\n",
      "cover with paper\n",
      "\n",
      "[Sense 8]\n",
      "SynLemmas\n",
      "[Lemma('wallpaper.v.01.wallpaper'), Lemma('wallpaper.v.01.paper')]\n",
      "Synonyms\n",
      "['wallpaper', 'paper']\n",
      "Definition\n",
      "cover with wallpaper\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can print the information for each sense of \"paper\"\n",
    "\n",
    "for i in range(len(paperSenses)):\n",
    "    print(\"[Sense \" + str(i) + \"]\")\n",
    "    printBasicSynsetInfo(paperSenses[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rich taxonomy has been manually developed in Wordnet, making it a rich resource.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO-DO : Q1)** Choose two words, and write code to print the taxonomic information for all senses of those words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a function to print the basic information, receives a synset\n",
    "\n",
    "def printTaxonomyInfo(d):\n",
    "    synonyms = [l.name() for l in d.lemmas()]\n",
    "    print(synonyms)\n",
    "    print(\"Hypernyms:\")\n",
    "    print(d.hypernyms())\n",
    "    print(\"Hyponyms:\")\n",
    "    print(d.hyponyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sense 0]\n",
      "['computer', 'computing_machine', 'computing_device', 'data_processor', 'electronic_computer', 'information_processing_system']\n",
      "Hypernyms:\n",
      "[Synset('machine.n.01')]\n",
      "Hyponyms:\n",
      "[Synset('analog_computer.n.01'), Synset('digital_computer.n.01'), Synset('home_computer.n.01'), Synset('node.n.08'), Synset('number_cruncher.n.02'), Synset('pari-mutuel_machine.n.01'), Synset('predictor.n.03'), Synset('server.n.03'), Synset('turing_machine.n.01'), Synset('web_site.n.01')]\n",
      "\n",
      "[Sense 1]\n",
      "['calculator', 'reckoner', 'figurer', 'estimator', 'computer']\n",
      "Hypernyms:\n",
      "[Synset('expert.n.01')]\n",
      "Hyponyms:\n",
      "[Synset('adder.n.01'), Synset('number_cruncher.n.01'), Synset('statistician.n.02'), Synset('subtracter.n.01')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q1 - ANSWER\n",
    "# We can print the taxonomy information for each sense of a word X\n",
    "ComputerSenses = wordnet.synsets('computer')\n",
    "for i in range(len(ComputerSenses)):\n",
    "    print(\"[Sense \" + str(i) + \"]\")\n",
    "    printTaxonomyInfo(ComputerSenses[i])\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sense 0]\n",
      "['flower']\n",
      "Hypernyms:\n",
      "[Synset('angiosperm.n.01')]\n",
      "Hyponyms:\n",
      "[Synset('achimenes.n.01'), Synset('african_daisy.n.01'), Synset('african_daisy.n.02'), Synset('african_daisy.n.03'), Synset('african_violet.n.01'), Synset('ageratum.n.02'), Synset('ammobium.n.01'), Synset('anemone.n.01'), Synset('aster.n.01'), Synset('baby's_breath.n.01'), Synset('bartonia.n.01'), Synset('begonia.n.01'), Synset('bellwort.n.01'), Synset('billy_buttons.n.01'), Synset('blazing_star.n.01'), Synset('bloomer.n.01'), Synset('blue-eyed_african_daisy.n.01'), Synset('blue_daisy.n.01'), Synset('brass_buttons.n.01'), Synset('bush_violet.n.01'), Synset('butterfly_flower.n.01'), Synset('calceolaria.n.01'), Synset('calendula.n.01'), Synset('calla_lily.n.01'), Synset('candytuft.n.01'), Synset('cape_marigold.n.01'), Synset('carolina_spring_beauty.n.01'), Synset('catananche.n.01'), Synset('centaury.n.01'), Synset('china_aster.n.01'), Synset('christmas_bells.n.01'), Synset('chrysanthemum.n.02'), Synset('cineraria.n.01'), Synset('columbine.n.01'), Synset('commelina.n.01'), Synset('composite.n.02'), Synset('coneflower.n.01'), Synset('coneflower.n.03'), Synset('coral_drops.n.01'), Synset('cornflower.n.02'), Synset('corydalis.n.01'), Synset('cosmos.n.02'), Synset('cotton_rose.n.02'), Synset('cowherb.n.01'), Synset('cyclamen.n.01'), Synset('dahlia.n.01'), Synset('daisy.n.01'), Synset('damask_violet.n.01'), Synset('delphinium.n.01'), Synset('easter_daisy.n.01'), Synset('fig_marigold.n.01'), Synset('florest's_cineraria.n.01'), Synset('four_o'clock.n.01'), Synset('gazania.n.01'), Synset('gentian.n.01'), Synset('gerardia.n.01'), Synset('globe_amaranth.n.01'), Synset('heliophila.n.01'), Synset('horn_poppy.n.01'), Synset('kingfisher_daisy.n.01'), Synset('lace-flower_vine.n.01'), Synset('lesser_celandine.n.01'), Synset('lychnis.n.01'), Synset('malcolm_stock.n.01'), Synset('marigold.n.01'), Synset('mexican_sunflower.n.01'), Synset('mistflower.n.01'), Synset('nigella.n.01'), Synset('orchid.n.01'), Synset('oxeye_daisy.n.02'), Synset('painted_daisy.n.01'), Synset('peony.n.01'), Synset('petunia.n.01'), Synset('pheasant's-eye.n.01'), Synset('pink.n.02'), Synset('poppy.n.01'), Synset('portulaca.n.01'), Synset('prairie_rocket.n.01'), Synset('prairie_rocket.n.02'), Synset('red_valerian.n.01'), Synset('rocket_larkspur.n.01'), Synset('rue_anemone.n.01'), Synset('sandwort.n.01'), Synset('sandwort.n.02'), Synset('sandwort.n.03'), Synset('scabious.n.01'), Synset('scarlet_musk_flower.n.01'), Synset('schizopetalon.n.01'), Synset('scorpionweed.n.01'), Synset('shortia.n.01'), Synset('silene.n.01'), Synset('snapdragon.n.01'), Synset('soapwort.n.01'), Synset('sowbread.n.01'), Synset('spathiphyllum.n.01'), Synset('spring_beauty.n.01'), Synset('stock.n.12'), Synset('stokes'_aster.n.01'), Synset('streptocarpus.n.01'), Synset('sunflower.n.01'), Synset('swan_river_daisy.n.01'), Synset('sweet_alyssum.n.01'), Synset('sweet_sultan.n.02'), Synset('sweet_sultan.n.03'), Synset('texas_star.n.02'), Synset('tidytips.n.01'), Synset('toadflax.n.01'), Synset('tuberose.n.01'), Synset('umbrellawort.n.01'), Synset('ursinia.n.01'), Synset('valerian.n.01'), Synset('verbena.n.01'), Synset('veronica.n.01'), Synset('virginia_spring_beauty.n.01'), Synset('virginian_stock.n.01'), Synset('wallflower.n.01'), Synset('wallflower.n.02'), Synset('wandflower.n.01'), Synset('western_wall_flower.n.01'), Synset('white-topped_aster.n.01'), Synset('woodland_star.n.01'), Synset('xeranthemum.n.01'), Synset('zinnia.n.01')]\n",
      "\n",
      "[Sense 1]\n",
      "['flower', 'bloom', 'blossom']\n",
      "Hypernyms:\n",
      "[Synset('reproductive_structure.n.01')]\n",
      "Hyponyms:\n",
      "[Synset('apetalous_flower.n.01'), Synset('bud.n.01'), Synset('chrysanthemum.n.01'), Synset('floret.n.01'), Synset('inflorescence.n.02'), Synset('ray_flower.n.01')]\n",
      "\n",
      "[Sense 2]\n",
      "['flower', 'prime', 'peak', 'heyday', 'bloom', 'blossom', 'efflorescence', 'flush']\n",
      "Hypernyms:\n",
      "[Synset('time_period.n.01')]\n",
      "Hyponyms:\n",
      "[Synset('golden_age.n.01')]\n",
      "\n",
      "[Sense 3]\n",
      "['bloom', 'blossom', 'flower']\n",
      "Hypernyms:\n",
      "[Synset('develop.v.10')]\n",
      "Hyponyms:\n",
      "[Synset('effloresce.v.01')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FlowerSenses = wordnet.synsets('flower')\n",
    "for i in range(len(FlowerSenses)):\n",
    "    print(\"[Sense \" + str(i) + \"]\")\n",
    "    printTaxonomyInfo(FlowerSenses[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Word-Sense Disambiguation.**  \n",
    "\n",
    "Let's now implement a simple modified Lesk algorithm for WSD.  \n",
    "The idea is to compare the sentence containing the ambiguous word W to all the definitions of W and choose the most similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Step 1) Create a BOW (bag of words) for each definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will need the tokenizer\n",
    "\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a small method to return the set of words found in a text\n",
    "# we can exclude some words\n",
    "\n",
    "def bow(text, excluded = None):\n",
    "    text = text.replace(\"_\", \" \") # the compound nouns in wordnet text have _\n",
    "    tokens = word_tokenize(text)\n",
    "    setTokens = set(tokens)\n",
    "    if excluded != None:\n",
    "        if (excluded in setTokens):\n",
    "            setTokens.remove(excluded)\n",
    "    return setTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'There', 'food', 'on', 'of', 'is', 'a', 'the', 'lot'}\n",
      "{'conference', 'by', 'researchers', 'excellent', 'an', 'He', 'many', 'wrote', 'referred'}\n"
     ]
    }
   ],
   "source": [
    "# testing \n",
    "print(bow(\"There is a lot of food on the table\", excluded='table'))\n",
    "print(bow(\"He wrote an excellent conference paper referred by many researchers\", excluded='paper'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make BOWs for all the senses in a received word\n",
    "# exclude from the BOW, the word being defined\n",
    "\n",
    "def makeDefBOWs(testWord):\n",
    "    synsets = wordnet.synsets(testWord)\n",
    "    defs = [s.definition() for s in synsets]\n",
    "    bows = [bow(d, excluded=testWord) for d in defs]\n",
    "    return bows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'any', 'compartment', 'small'}\n",
      "{')', '(', 'all', 'or', 'tissues', 'functional', 'higher', 'of', 'as', 'unit', 'the', 'basic', 'may', 'exist', 'animals', ';', 'they', 'plants', 'form', 'units', 'independent', 'in', 'and', 'life', 'structural', 'monads', 'biology', 'colonies', 'organisms'}\n",
      "{'delivers', 'chemical', 'as', 'of', 'a', 'the', 'electric', 'reaction', 'result', 'current', 'that', 'an', 'device'}\n",
      "{'small', 'movement', 'as', 'of', 'a', 'unit', 'the', 'larger', 'part', 'nucleus', 'or', 'serving', 'political'}\n",
      "{',', 'short-range', 'radiotelephone', 'hand-held', 'in', 'divided', 'into', 'use', 'small', 'sections', 'with', 'a', 'its', 'mobile', 'own', 'transmitter/receiver', 'an', 'for', 'area', 'each'}\n",
      "{'small', 'which', 'in', 'lives', 'a', 'room', 'or', 'monk', 'nun'}\n",
      "{'is', 'kept', 'a', 'room', 'prisoner', 'where'}\n"
     ]
    }
   ],
   "source": [
    "# try with different words, look at the resulting info\n",
    "\n",
    "testWord = \"cell\" # bank, course, paper, ...\n",
    "defBOWs = makeDefBOWs(testWord)\n",
    "    \n",
    "print(*defBOWs, sep=\"\\n\")  # to print a list on separate lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Step 2) Create a method to compare BOWs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're interested in the size of the intersection between the BOWs\n",
    "# If you wish to see the words in common to understand the results, uncomment the prints\n",
    "\n",
    "def bowOverlap(bow1, bow2):\n",
    "    #print(bow1)\n",
    "    #print(bow2)\n",
    "    print(bow1.intersection(bow2))\n",
    "    return len(bow1.intersection(bow2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO-DO: Q2)** Implement the (Step 3) of the algorithm.  The (Step 3) consist in comparing the BOW of a test sentence (let's call it our context C) containing an ambiguous word (X) to the BOWs of all the senses of the X.  To do Step 3, you need to complete the method below which receives a word X, as well as the text C in which X occurs.  The method should return the synsets with largest common BOWs with X.  Notice that there could be more than one maximum, so your method should return all synsets with maximum intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2 - ANSWER\n",
    "\n",
    "# method receives a word and its context\n",
    "# returns all the synsets with maximum overlap\n",
    "\n",
    "def findMostProbableSense(word, context):\n",
    "    bows = makeDefBOWs(word)\n",
    "    textBOW = bow(context)\n",
    "    maxSynet = []\n",
    "    maxSame = 0\n",
    "    for bag in bows:\n",
    "        numSame = bowOverlap(textBOW, bag)\n",
    "        if(numSame >= maxSame):\n",
    "            for word in textBOW.intersection(bag):\n",
    "                maxSynet.append(wordnet.synsets(word))\n",
    "            if(numSame > maxSame):\n",
    "                maxSame = numSame\n",
    "    return maxSynet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Your method should return the chosen senses for the example below.  We will test your method using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "{'in'}\n",
      "set()\n",
      "set()\n",
      "{'for', 'in'}\n",
      "{'in'}\n",
      "set()\n",
      "Synset('inch.n.01')\n",
      "SynLemmas\n",
      "[Lemma('inch.n.01.inch'), Lemma('inch.n.01.in')]\n",
      "Synonyms\n",
      "['inch', 'in']\n",
      "Definition\n",
      "a unit of length equal to one twelfth of a foot\n",
      "Synset('indium.n.01')\n",
      "SynLemmas\n",
      "[Lemma('indium.n.01.indium'), Lemma('indium.n.01.In'), Lemma('indium.n.01.atomic_number_49')]\n",
      "Synonyms\n",
      "['indium', 'In', 'atomic_number_49']\n",
      "Definition\n",
      "a rare soft silvery metallic element; occurs in small quantities in sphalerite\n",
      "Synset('indiana.n.01')\n",
      "SynLemmas\n",
      "[Lemma('indiana.n.01.Indiana'), Lemma('indiana.n.01.Hoosier_State'), Lemma('indiana.n.01.IN')]\n",
      "Synonyms\n",
      "['Indiana', 'Hoosier_State', 'IN']\n",
      "Definition\n",
      "a state in midwestern United States\n",
      "Synset('in.s.01')\n",
      "SynLemmas\n",
      "[Lemma('in.s.01.in')]\n",
      "Synonyms\n",
      "['in']\n",
      "Definition\n",
      "holding office\n",
      "Synset('in.s.02')\n",
      "SynLemmas\n",
      "[Lemma('in.s.02.in')]\n",
      "Synonyms\n",
      "['in']\n",
      "Definition\n",
      "directed or bound inward\n",
      "Synset('in.s.03')\n",
      "SynLemmas\n",
      "[Lemma('in.s.03.in')]\n",
      "Synonyms\n",
      "['in']\n",
      "Definition\n",
      "currently fashionable\n",
      "Synset('in.r.01')\n",
      "SynLemmas\n",
      "[Lemma('in.r.01.in'), Lemma('in.r.01.inwards'), Lemma('in.r.01.inward')]\n",
      "Synonyms\n",
      "['in', 'inwards', 'inward']\n",
      "Definition\n",
      "to or toward the inside of\n",
      "Synset('inch.n.01')\n",
      "SynLemmas\n",
      "[Lemma('inch.n.01.inch'), Lemma('inch.n.01.in')]\n",
      "Synonyms\n",
      "['inch', 'in']\n",
      "Definition\n",
      "a unit of length equal to one twelfth of a foot\n",
      "Synset('indium.n.01')\n",
      "SynLemmas\n",
      "[Lemma('indium.n.01.indium'), Lemma('indium.n.01.In'), Lemma('indium.n.01.atomic_number_49')]\n",
      "Synonyms\n",
      "['indium', 'In', 'atomic_number_49']\n",
      "Definition\n",
      "a rare soft silvery metallic element; occurs in small quantities in sphalerite\n",
      "Synset('indiana.n.01')\n",
      "SynLemmas\n",
      "[Lemma('indiana.n.01.Indiana'), Lemma('indiana.n.01.Hoosier_State'), Lemma('indiana.n.01.IN')]\n",
      "Synonyms\n",
      "['Indiana', 'Hoosier_State', 'IN']\n",
      "Definition\n",
      "a state in midwestern United States\n",
      "Synset('in.s.01')\n",
      "SynLemmas\n",
      "[Lemma('in.s.01.in')]\n",
      "Synonyms\n",
      "['in']\n",
      "Definition\n",
      "holding office\n",
      "Synset('in.s.02')\n",
      "SynLemmas\n",
      "[Lemma('in.s.02.in')]\n",
      "Synonyms\n",
      "['in']\n",
      "Definition\n",
      "directed or bound inward\n",
      "Synset('in.s.03')\n",
      "SynLemmas\n",
      "[Lemma('in.s.03.in')]\n",
      "Synonyms\n",
      "['in']\n",
      "Definition\n",
      "currently fashionable\n",
      "Synset('in.r.01')\n",
      "SynLemmas\n",
      "[Lemma('in.r.01.in'), Lemma('in.r.01.inwards'), Lemma('in.r.01.inward')]\n",
      "Synonyms\n",
      "['in', 'inwards', 'inward']\n",
      "Definition\n",
      "to or toward the inside of\n"
     ]
    }
   ],
   "source": [
    "# Show the BOWs of the senses with the overlap, and the chosen sense(s)\n",
    "# You can try with various words and sentences\n",
    "\n",
    "testWord = \"cell\"\n",
    "testSentence = \"He lived in this prison cell for many years.\"\n",
    "\n",
    "\n",
    "####  CALL TO YOUR METHOD RECEIVING THE WORD AND ITS CONTEXT\n",
    "chosenSynsets = findMostProbableSense(testWord, testSentence)  \n",
    "\n",
    "# print all the definitions of the most probable senses\n",
    "for s in chosenSynsets:\n",
    "    for x in s:\n",
    "        print(x)\n",
    "        printBasicSynsetInfo(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO-DO: Q3)** What do you notice? With the example above for \"cell\", what are the words making the BOWs look similar?  Are these significant words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Q3-ANSWER*  \n",
    "They are all looking like \"in\" such as \"inch\" \"indiana\" and \"inch\".  They are not significant words in this case. it seems like it has focused on the wrong word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO-DO: Q4)  Refining our BOWs**\n",
    "\n",
    "**Exploring variations:**\n",
    "1. What if you lowercase everything?\n",
    "2. What if you apply lemmatisation on all words in the BOWs?\n",
    "3. What if you focus on only the NOUNS in the BOWs?\n",
    "\n",
    "(hint) Go back to your notebook NLP pipeline for questions (2) use the lemmatizer and (3) perform POS tagging on the sentences. \n",
    "\n",
    "For your answer (code to write):  \n",
    "\n",
    "a) First complete the BOW method below in which I've added parameters to possibly activate the lowercase, the lemmatization and the POS tagging.   \n",
    "b) Add a few tests to see if your BOW works.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/FLSingerman/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Q4 - ANSWER - part a)\n",
    "\n",
    "# The parameters possibly ACTIVATE lowercase, lemmatization, and keeping only Nouns in BOWs.\n",
    "\n",
    "# nltk contains a method to obtain the part-of-speech of each token\n",
    "# Download the wordnet resource\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.ADV  # just use as default, for ADV the lemmatizer doesn't change anything \n",
    "\n",
    "# refine the method with parameters\n",
    "def bow(text, excluded = None, lowercase = False, lemmatize=False, nounsOnly=False):\n",
    "    text = text.replace(\"_\", \" \")\n",
    "    if lowercase:\n",
    "        text = text.lower()     \n",
    "    tokens = word_tokenize(text)\n",
    "    setTokens = set(tokens)\n",
    "    if excluded != None:\n",
    "        if (excluded in setTokens):\n",
    "            setTokens.remove(excluded)\n",
    "    if lemmatize: \n",
    "        setTokens = [wnl.lemmatize(t) for t in setTokens]\n",
    "        setTokens = set(setTokens)\n",
    "    if nounsOnly:\n",
    "        nounToken = []\n",
    "        posTokens = nltk.pos_tag(setTokens)\n",
    "        wordnet_tags = [get_wordnet_pos(p[1]) for p in posTokens]\n",
    "        for idx, val in enumerate(setTokens):\n",
    "            if wordnet_tags[idx] == 'n':\n",
    "                nounToken.append(val)\n",
    "        setTokens = set(nounToken)\n",
    "    return setTokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'food', 'lot'}\n",
      "{'love', 'I', 'the', 'Eagles', 'Philadelphia'}\n",
      "{'hope', 'course', 'i', 'exam'}\n"
     ]
    }
   ],
   "source": [
    "# Q4 - ANSWER - part b)\n",
    "\n",
    "# TEST YOUR METHOD \n",
    "print(bow(\"There is a lot of food on the table\", excluded='table', lowercase=True, lemmatize=True, nounsOnly=True))\n",
    "# Your example 1\n",
    "print(bow(\"I love the Philadelphia Eagles\", excluded=None, lowercase=False, lemmatize=False, nounsOnly=False))\n",
    "# Your example 2\n",
    "print(bow(\"I hope I do well on this assignment. I hope I did well on the exams. I hope I do well in the course\", \n",
    "          excluded='well', lowercase=True, lemmatize=True, nounsOnly=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO-DO: Q5)** TESTING BOW VARIATIONS IN LESK-LIKE DISAMBIGUATION\n",
    "\n",
    "a) Redo the method makeDefBOW and findMostProbableSense to use the new parameters.  \n",
    "\n",
    "b) Generate three example cases and test your disambiguation strategy programmed above.  An example case contains an ambiguous word (e.g. bank) and a sentence in which that word must be disambiguated (e.g. He sat on the bank throwing rocks in the water.).  \n",
    "\n",
    "c) For your examples, which filtering seems to work better (with/without lemmatization, with/without focus only on nouns)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5 - ANSWER - part a)\n",
    "\n",
    "# add the parameters to makeBOW as well, same default\n",
    "def makeDefBOWs(testWord, lowercase=False, lemmatize=False, nounsOnly=False):\n",
    "    if lowercase:\n",
    "        testWord = testWord.lower()     \n",
    "    excluded = testWord\n",
    "    synsets = wordnet.synsets(testWord)\n",
    "    defs = [s.definition() for s in synsets]\n",
    "    for d in defs:\n",
    "        bows = bow(d, excluded, lowercase, lemmatize, nounsOnly)\n",
    "    #bows = [bow(d, excluded=testWord, lowecase, lemmatize, nounsOnly) for d in defs]\n",
    "    return bows\n",
    "\n",
    "#     if lemmatize: \n",
    "#         setTokens = [wnl.lemmatize(t) for t in setTokens]\n",
    "#         setTokens = set(setTokens)\n",
    "#     if nounsOnly:\n",
    "#         nounToken = []\n",
    "#         posTokens = nltk.pos_tag(setTokens)\n",
    "#         wordnet_tags = [get_wordnet_pos(p[1]) for p in posTokens]\n",
    "#         for idx, val in enumerate(setTokens):\n",
    "#             if wordnet_tags[idx] == 'n':\n",
    "#                 nounToken.append(val)\n",
    "#         setTokens = set(nounToken)\n",
    "\n",
    "from nltk.stem import *\n",
    "from nltk.stem.porter import *\n",
    "        \n",
    "from nltk.corpus import wordnet\n",
    "def findMostProbableSense(word, text, stemming=False, lowercase=False, lemmatize=False, nounsOnly=False):\n",
    "    if stemming:\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens_word = word_tokenize(text)\n",
    "        stemmer = PorterStemmer()\n",
    "        singles = [stemmer.stem(t) for t in tokens]\n",
    "        singles_word = [stemmer.stem(t) for t in tokens_word]\n",
    "        textBOW = bow(singles,lowercase, lemmatize, nounsOnly)\n",
    "        bows = makeDefBOWs(singles_word, lowercase, lemmatize, nounsOnly)\n",
    "    else:    \n",
    "        bows = makeDefBOWs(word, lowercase, lemmatize, nounsOnly)\n",
    "        textBOW = bow(text, lowercase, lemmatize, nounsOnly)\n",
    "    maxSynet = []\n",
    "    maxSame = 0\n",
    "    for bag in bows:\n",
    "        numSame = bowOverlap(textBOW, bag)\n",
    "        if(numSame >= maxSame):\n",
    "            for word in textBOW.intersection(bag):\n",
    "                maxSynet.append(wordnet.synsets(word))\n",
    "            if(numSame > maxSame):\n",
    "                maxSame = numSame\n",
    "    return maxSynet\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # also add the parameter here, copy your method from above and add a parameter for stemming\n",
    "# def findMostProbableSense(senses, text, stemming=False):\n",
    "\n",
    "#     maxSynet = []\n",
    "#     maxSame = 0\n",
    "#     for bag in bows:\n",
    "#         numSame = bowOverlap(textBOW, bag)\n",
    "#         if(numSame >= maxSame):\n",
    "#             for word in textBOW.intersection(bag):\n",
    "#                 maxSynet.append(wordnet.synsets(word))\n",
    "#             if(numSame > maxSame):\n",
    "#                 maxSame = numSame\n",
    "#     return maxSynet\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "{'a'}\n",
      "set()\n",
      "SynLemmas\n",
      "[Lemma('angstrom.n.01.angstrom'), Lemma('angstrom.n.01.angstrom_unit'), Lemma('angstrom.n.01.A')]\n",
      "Synonyms\n",
      "['angstrom', 'angstrom_unit', 'A']\n",
      "Definition\n",
      "a metric unit of length equal to one ten billionth of a meter (or 0.0001 micron); used to specify wavelengths of electromagnetic radiation\n",
      "SynLemmas\n",
      "[Lemma('vitamin_a.n.01.vitamin_A'), Lemma('vitamin_a.n.01.antiophthalmic_factor'), Lemma('vitamin_a.n.01.axerophthol'), Lemma('vitamin_a.n.01.A')]\n",
      "Synonyms\n",
      "['vitamin_A', 'antiophthalmic_factor', 'axerophthol', 'A']\n",
      "Definition\n",
      "any of several fat-soluble vitamins essential for normal vision; prevents night blindness or inflammation or dryness of the eyes\n",
      "SynLemmas\n",
      "[Lemma('deoxyadenosine_monophosphate.n.01.deoxyadenosine_monophosphate'), Lemma('deoxyadenosine_monophosphate.n.01.A')]\n",
      "Synonyms\n",
      "['deoxyadenosine_monophosphate', 'A']\n",
      "Definition\n",
      "one of the four nucleotides used in building DNA; all four nucleotides have a common phosphate group and a sugar (ribose)\n",
      "SynLemmas\n",
      "[Lemma('adenine.n.01.adenine'), Lemma('adenine.n.01.A')]\n",
      "Synonyms\n",
      "['adenine', 'A']\n",
      "Definition\n",
      "(biochemistry) purine base found in DNA and RNA; pairs with thymine in DNA and with uracil in RNA\n",
      "SynLemmas\n",
      "[Lemma('ampere.n.02.ampere'), Lemma('ampere.n.02.amp'), Lemma('ampere.n.02.A')]\n",
      "Synonyms\n",
      "['ampere', 'amp', 'A']\n",
      "Definition\n",
      "the basic unit of electric current adopted under the Systeme International d'Unites\n",
      "SynLemmas\n",
      "[Lemma('a.n.06.A'), Lemma('a.n.06.a')]\n",
      "Synonyms\n",
      "['A', 'a']\n",
      "Definition\n",
      "the 1st letter of the Roman alphabet\n",
      "SynLemmas\n",
      "[Lemma('a.n.07.A'), Lemma('a.n.07.type_A'), Lemma('a.n.07.group_A')]\n",
      "Synonyms\n",
      "['A', 'type_A', 'group_A']\n",
      "Definition\n",
      "the blood group whose red cells carry the A antigen\n",
      "set()\n",
      "set()\n",
      "******* lowercase=True, lemmatize=False, nounsOnly=True\n",
      "{'i'}\n",
      "set()\n",
      "{'i'}\n",
      "{'i'}\n",
      "****** lowercase=True, lemmatize=True, nounsOnly=False\n",
      "SynLemmas\n",
      "[Lemma('iodine.n.01.iodine'), Lemma('iodine.n.01.iodin'), Lemma('iodine.n.01.I'), Lemma('iodine.n.01.atomic_number_53')]\n",
      "Synonyms\n",
      "['iodine', 'iodin', 'I', 'atomic_number_53']\n",
      "Definition\n",
      "a nonmetallic element belonging to the halogens; used especially in medicine and photography and in dyes; occurs naturally only in combination in small quantities (as in sea water or rocks)\n",
      "SynLemmas\n",
      "[Lemma('one.n.01.one'), Lemma('one.n.01.1'), Lemma('one.n.01.I'), Lemma('one.n.01.ace'), Lemma('one.n.01.single'), Lemma('one.n.01.unity')]\n",
      "Synonyms\n",
      "['one', '1', 'I', 'ace', 'single', 'unity']\n",
      "Definition\n",
      "the smallest whole number or a numeral representing this number\n",
      "SynLemmas\n",
      "[Lemma('i.n.03.I'), Lemma('i.n.03.i')]\n",
      "Synonyms\n",
      "['I', 'i']\n",
      "Definition\n",
      "the 9th letter of the Roman alphabet\n",
      "SynLemmas\n",
      "[Lemma('one.s.01.one'), Lemma('one.s.01.1'), Lemma('one.s.01.i'), Lemma('one.s.01.ane')]\n",
      "Synonyms\n",
      "['one', '1', 'i', 'ane']\n",
      "Definition\n",
      "used of a single unit or thing; not two or more\n",
      "SynLemmas\n",
      "[Lemma('iodine.n.01.iodine'), Lemma('iodine.n.01.iodin'), Lemma('iodine.n.01.I'), Lemma('iodine.n.01.atomic_number_53')]\n",
      "Synonyms\n",
      "['iodine', 'iodin', 'I', 'atomic_number_53']\n",
      "Definition\n",
      "a nonmetallic element belonging to the halogens; used especially in medicine and photography and in dyes; occurs naturally only in combination in small quantities (as in sea water or rocks)\n",
      "SynLemmas\n",
      "[Lemma('one.n.01.one'), Lemma('one.n.01.1'), Lemma('one.n.01.I'), Lemma('one.n.01.ace'), Lemma('one.n.01.single'), Lemma('one.n.01.unity')]\n",
      "Synonyms\n",
      "['one', '1', 'I', 'ace', 'single', 'unity']\n",
      "Definition\n",
      "the smallest whole number or a numeral representing this number\n",
      "SynLemmas\n",
      "[Lemma('i.n.03.I'), Lemma('i.n.03.i')]\n",
      "Synonyms\n",
      "['I', 'i']\n",
      "Definition\n",
      "the 9th letter of the Roman alphabet\n",
      "SynLemmas\n",
      "[Lemma('one.s.01.one'), Lemma('one.s.01.1'), Lemma('one.s.01.i'), Lemma('one.s.01.ane')]\n",
      "Synonyms\n",
      "['one', '1', 'i', 'ane']\n",
      "Definition\n",
      "used of a single unit or thing; not two or more\n",
      "SynLemmas\n",
      "[Lemma('iodine.n.01.iodine'), Lemma('iodine.n.01.iodin'), Lemma('iodine.n.01.I'), Lemma('iodine.n.01.atomic_number_53')]\n",
      "Synonyms\n",
      "['iodine', 'iodin', 'I', 'atomic_number_53']\n",
      "Definition\n",
      "a nonmetallic element belonging to the halogens; used especially in medicine and photography and in dyes; occurs naturally only in combination in small quantities (as in sea water or rocks)\n",
      "SynLemmas\n",
      "[Lemma('one.n.01.one'), Lemma('one.n.01.1'), Lemma('one.n.01.I'), Lemma('one.n.01.ace'), Lemma('one.n.01.single'), Lemma('one.n.01.unity')]\n",
      "Synonyms\n",
      "['one', '1', 'I', 'ace', 'single', 'unity']\n",
      "Definition\n",
      "the smallest whole number or a numeral representing this number\n",
      "SynLemmas\n",
      "[Lemma('i.n.03.I'), Lemma('i.n.03.i')]\n",
      "Synonyms\n",
      "['I', 'i']\n",
      "Definition\n",
      "the 9th letter of the Roman alphabet\n",
      "SynLemmas\n",
      "[Lemma('one.s.01.one'), Lemma('one.s.01.1'), Lemma('one.s.01.i'), Lemma('one.s.01.ane')]\n",
      "Synonyms\n",
      "['one', '1', 'i', 'ane']\n",
      "Definition\n",
      "used of a single unit or thing; not two or more\n",
      "{'i'}\n",
      "set()\n",
      "{'i'}\n",
      "******** lowercase=True, lemmatize=True, nounsOnly=True\n",
      "SynLemmas\n",
      "[Lemma('iodine.n.01.iodine'), Lemma('iodine.n.01.iodin'), Lemma('iodine.n.01.I'), Lemma('iodine.n.01.atomic_number_53')]\n",
      "Synonyms\n",
      "['iodine', 'iodin', 'I', 'atomic_number_53']\n",
      "Definition\n",
      "a nonmetallic element belonging to the halogens; used especially in medicine and photography and in dyes; occurs naturally only in combination in small quantities (as in sea water or rocks)\n",
      "SynLemmas\n",
      "[Lemma('one.n.01.one'), Lemma('one.n.01.1'), Lemma('one.n.01.I'), Lemma('one.n.01.ace'), Lemma('one.n.01.single'), Lemma('one.n.01.unity')]\n",
      "Synonyms\n",
      "['one', '1', 'I', 'ace', 'single', 'unity']\n",
      "Definition\n",
      "the smallest whole number or a numeral representing this number\n",
      "SynLemmas\n",
      "[Lemma('i.n.03.I'), Lemma('i.n.03.i')]\n",
      "Synonyms\n",
      "['I', 'i']\n",
      "Definition\n",
      "the 9th letter of the Roman alphabet\n",
      "SynLemmas\n",
      "[Lemma('one.s.01.one'), Lemma('one.s.01.1'), Lemma('one.s.01.i'), Lemma('one.s.01.ane')]\n",
      "Synonyms\n",
      "['one', '1', 'i', 'ane']\n",
      "Definition\n",
      "used of a single unit or thing; not two or more\n",
      "SynLemmas\n",
      "[Lemma('iodine.n.01.iodine'), Lemma('iodine.n.01.iodin'), Lemma('iodine.n.01.I'), Lemma('iodine.n.01.atomic_number_53')]\n",
      "Synonyms\n",
      "['iodine', 'iodin', 'I', 'atomic_number_53']\n",
      "Definition\n",
      "a nonmetallic element belonging to the halogens; used especially in medicine and photography and in dyes; occurs naturally only in combination in small quantities (as in sea water or rocks)\n",
      "SynLemmas\n",
      "[Lemma('one.n.01.one'), Lemma('one.n.01.1'), Lemma('one.n.01.I'), Lemma('one.n.01.ace'), Lemma('one.n.01.single'), Lemma('one.n.01.unity')]\n",
      "Synonyms\n",
      "['one', '1', 'I', 'ace', 'single', 'unity']\n",
      "Definition\n",
      "the smallest whole number or a numeral representing this number\n",
      "SynLemmas\n",
      "[Lemma('i.n.03.I'), Lemma('i.n.03.i')]\n",
      "Synonyms\n",
      "['I', 'i']\n",
      "Definition\n",
      "the 9th letter of the Roman alphabet\n",
      "SynLemmas\n",
      "[Lemma('one.s.01.one'), Lemma('one.s.01.1'), Lemma('one.s.01.i'), Lemma('one.s.01.ane')]\n",
      "Synonyms\n",
      "['one', '1', 'i', 'ane']\n",
      "Definition\n",
      "used of a single unit or thing; not two or more\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "******** lowercase=True, lemmatize=flase, nounsOnly=false\n"
     ]
    }
   ],
   "source": [
    "# Q5 - ANSWER - part b)\n",
    "\n",
    "testWord = \"table\"\n",
    "testSentence = \"There is a lot of food on the table.\"\n",
    "chosenSynsets = findMostProbableSense(testWord, testSentence, lowercase=True, lemmatize=True, nounsOnly=True)  \n",
    "\n",
    "# print all the definitions of the most probable senses\n",
    "for s in chosenSynsets:\n",
    "    for x in s:\n",
    "        printBasicSynsetInfo(x)\n",
    "    \n",
    "# Your example 1\n",
    "myWord = \"Mark\"\n",
    "mySentence = \"Mark my word, I will seek vengence.\"\n",
    "chosenSynsets = findMostProbableSense(myWord, mySentence, lowercase=True, lemmatize=False, nounsOnly=True)  \n",
    "\n",
    "# print all the definitions of the most probable senses\n",
    "print(\"******* lowercase=True, lemmatize=False, nounsOnly=True\")\n",
    "for s in chosenSynsets:\n",
    "    for x in s:\n",
    "        printBasicSynsetInfo(x)\n",
    "\n",
    "\n",
    "# Your example 2\n",
    "chosenSynsets = findMostProbableSense(myWord, mySentence, lowercase=True, lemmatize=True, nounsOnly=False)  \n",
    "\n",
    "# print all the definitions of the most probable senses\n",
    "print(\"****** lowercase=True, lemmatize=True, nounsOnly=False\")\n",
    "for s in chosenSynsets:\n",
    "    for x in s:\n",
    "        printBasicSynsetInfo(x)\n",
    "\n",
    "# Your example 3\n",
    "chosenSynsets = findMostProbableSense(myWord, mySentence, lowercase=True, lemmatize=True, nounsOnly=True)  \n",
    "\n",
    "# print all the definitions of the most probable senses\n",
    "print(\"******** lowercase=True, lemmatize=True, nounsOnly=True\")\n",
    "for s in chosenSynsets:\n",
    "    for x in s:\n",
    "        \n",
    "        printBasicSynsetInfo(x)\n",
    "\n",
    "        \n",
    "chosenSynsets = findMostProbableSense(myWord, mySentence, lowercase=True, lemmatize=False, nounsOnly=False)  \n",
    "\n",
    "# print all the definitions of the most probable senses\n",
    "print(\"******** lowercase=True, lemmatize=flase, nounsOnly=false\")\n",
    "for s in chosenSynsets:\n",
    "    for x in s:\n",
    "        \n",
    "        printBasicSynsetInfo(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Q5 - ANSWER - part c)* The best filtering seems to be with/without lemmatization and with/without nounsonly\n",
    "\n",
    "Nouns only alone does not work well at all. Using both lemmatize and nouns did not seem to give me great results. Using both lematize and nouns seems to have given me the same results as just lemmatize and they are still not good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signature\n",
    "\n",
    "I, Felix Singerman, declare that the answers provided in this notebook are my own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
