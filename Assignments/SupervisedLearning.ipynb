{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning\n",
    "### Experimental set-up and Naive Bayes for Polarity Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSI4106 Artificial Intelligence  \n",
    "Fall 2018  \n",
    "Caroline BarriÃ¨re\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The supervised classification task tackled in this notebook is **polarity detection**, which is one possible activity within the quite popular trend of *Opinion Mining* in AI.  Many companies want to know whether there are positive or negative reviews about them.  Reviews can be on hotels, restaurants, movies, customer service of any kind, etc.\n",
    "\n",
    "This notebook will allow you to better understand an ***experimental set-up*** for supervised machine learning.  The notion of training set, test set, evaluation, bias, etc.  The notebook also introduces the notion of comparative evaluation.  To say if a method is good or not, we often compare it to a *baseline* approach.  \n",
    "\n",
    "This notebook makes use of a really nice and popular machine learning package, called **scikit-learn** (http://scikit-learn.org/stable/).  It contains many pre-coded machine learning algorithms which you can call.  To use this package, you must download it.  At the command prompt, type *pip install sklearn* to download the package.  \n",
    "\n",
    "In this notebook we will use the Naive Bayes implementation, but we will probably explore other ML algorithms included in scikit-learn in future notebooks.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***HOMEWORK***:  \n",
    "Go through the notebook by running each cell, one at a time. Look for (**TO DO**) for the tasks that you need to perform.  \n",
    "Make sure you *sign* (type your name) the notebook at the end. Once you're done, submit your notebook.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Polarity detection**  \n",
    "\n",
    "In polarity detection, we use two classes: positive and negative.  This is different from sentiment analysis for example, in which the classes might be (sad, happy, anxious, angry, etc).  It's also more restricted than *rating* in which we would like assign a value (0..5) to evaluate a particular service.  So, the polarity detection task aims to assign either *positive* or *negative* to a statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Application domain:  Movie reviews**  \n",
    "\n",
    "Polarity detection could be used on reviews of anything.  In this notebook, we wish to apply polarity detection within the domain of movies.  Movie viewers are asked for comments about movies.  For our experimental set-up, we will first built a *test set* of reviews.  We manually determine the polarity of these reviews.  We **SHOULD NOT** use this test set to build our model later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's establish a few test sentences. \n",
    "# For each sentence, we have a corresponding tag: \"Neg\" or \"Pos\"\n",
    "\n",
    "test_reviews = [\"Can't believe I wasted my time on this\", \n",
    "                \"Really awful\",\n",
    "                \"Actors were good\",\n",
    "                \"So boring\",\n",
    "                \"Stayed at the edge of my seat\",\n",
    "                \"I never like any movie\",\n",
    "                \"Argghhhh I hated it\"]\n",
    "\n",
    "test_tags = [\"Neg\", \"Neg\", \"Pos\", \"Neg\", \"Pos\", \"Neg\", \"Neg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Bias:  Available resources**  \n",
    "\n",
    "For polarity detection, some researchers have established lists of positive and negative words.  The ones used in this notebook have been downloaded from [here](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html) (a website on Opinion Mining by renowned research Bing Lu) and stored locally.  The files *positive-words.txt* and *negative-words.txt* are in the Jupyter Notebook module in Brightspace.  Make sure you place these files in the same repertory as your notebook, or otherwise, within the code below, specify the path where to find the files on your disk.\n",
    "\n",
    "As we discussed in class, using any external resource is somewhat of a *bias* that we introduce in the study of a problem. Although in this particular case, the lists themselves have been compiled from data by other researchers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a+', 'abound', 'abounds', 'abundance', 'abundant', 'accessable', 'accessible', 'acclaim', 'acclaimed', 'acclamation', 'accolade', 'accolades', 'accommodative', 'accomodative', 'accomplish', 'accomplished', 'accomplishment', 'accomplishments', 'accurate', 'accurately', 'achievable', 'achievement', 'achievements', 'achievible', 'acumen', 'adaptable', 'adaptive', 'adequate', 'adjustable', 'admirable', 'admirably', 'admiration', 'admire', 'admirer', 'admiring', 'admiringly', 'adorable', 'adore', 'adored', 'adorer', 'adoring', 'adoringly', 'adroit', 'adroitly', 'adulate', 'adulation', 'adulatory', 'advanced', 'advantage', 'advantageous']\n"
     ]
    }
   ],
   "source": [
    "# Read the positive words\n",
    "\n",
    "with open(\"positive-words.txt\") as f:\n",
    "    posWords = f.readlines()\n",
    "posWords = [p[0:len(p)-1] for p in posWords if p[0].isalpha()] \n",
    "\n",
    "# print the first 50 words\n",
    "print(posWords[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abnormal', 'abolish', 'abominable', 'abominably', 'abominate', 'abomination', 'abort', 'aborted', 'aborts', 'abrade', 'abrasive', 'abrupt', 'abruptly', 'abscond', 'absence', 'absent-minded', 'absentee', 'absurd', 'absurdity', 'absurdly', 'absurdness', 'abuse', 'abused', 'abuses', 'abusive', 'abysmal', 'abysmally', 'abyss', 'accidental', 'accost', 'accursed', 'accusation', 'accusations', 'accuse', 'accuses', 'accusing', 'accusingly', 'acerbate', 'acerbic', 'acerbically', 'ache', 'ached', 'aches', 'achey', 'aching', 'acrid', 'acridly', 'acridness', 'acrimonious', 'acrimoniously']\n"
     ]
    }
   ],
   "source": [
    "# Read the negative words\n",
    "\n",
    "with open(\"negative-words.txt\", encoding = \"ISO-8859-1\") as f:\n",
    "    negWords = f.readlines()\n",
    "negWords = [p[0:len(p)-1] for p in negWords if p[0].isalpha()] \n",
    "\n",
    "print(negWords[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Baseline approach**  \n",
    "\n",
    "Before we evaluate the performances of a supervised learning approach, we can start by establishing a very simple baseline approach.  It's always good to start simple.  A baseline allows us to measure whether the additional complexity of the various models we develop is worth it or not.\n",
    "\n",
    "The *baseline algorithm* we will use simply counts the number of positive and negative words in the review and outputs the category corresponding to the maximum.  This approach DOES NOT LEARN anything.  It just uses a particular *reasoning* (strategy at test time).  You might be surprised to find out how many *AI start-ups* within the area of Opinion Mining, do use this kind of simple approach.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's define methods to count positive and negative words\n",
    "\n",
    "def countPos(text):\n",
    "    count = 0\n",
    "    for t in text.split():\n",
    "        if t in posWords:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def countNeg(text):\n",
    "    count = 0\n",
    "    for t in text.split():\n",
    "        if t in negWords:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple counting algorithm as baseline approach to polarity detection\n",
    "def baselinePolarity(review):\n",
    "    numPos = countPos(review)\n",
    "    numNeg = countNeg(review)\n",
    "    if numPos > numNeg:\n",
    "        return \"Pos\"   \n",
    "    else:\n",
    "        return \"Neg\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos\n"
     ]
    }
   ],
   "source": [
    "# Test the baseline method\n",
    "print(baselinePolarity(\"This was a really good movie\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Evaluation**  \n",
    "We saw in class that there could be multiple ways of evaluating an algorithm.  In the case of classification, a common evaluation method is simply to calculate *number of wrong choices*.\n",
    "\n",
    "To test our *baseline algorithm* we use the test set, defined earlier and calculate the number of wrong assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't believe I wasted my time on this -- Neg\n",
      "Really awful -- Neg\n",
      "Actors were good -- Pos\n",
      "So boring -- Neg\n",
      "Stayed at the edge of my seat -- Neg\n",
      "I never like any movie -- Pos\n",
      "Argghhhh I hated it -- Neg\n",
      "\n",
      "There are 2 wrong assignments\n"
     ]
    }
   ],
   "source": [
    "# Let's establish the polarity for each review\n",
    "\n",
    "nbWrong = 0\n",
    "for i in range(len(test_reviews)):\n",
    "    polarity = baselinePolarity(test_reviews[i])\n",
    "    print(test_reviews[i] + \" -- \" + polarity)\n",
    "    if (polarity != test_tags[i]):\n",
    "        nbWrong += 1\n",
    "\n",
    "print('\\nThere are %s wrong assignments' %nbWrong)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO - Q1)** Make a small test set with another 6 sentences of your choice.  Then rerun the baseline polarity algorithm, and determine how many wrong assignments it makes.  For the number of wrong assignments, you can simply copy the code from the previous cell and slightly modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg\n",
      "favourite movie ever -- Neg\n",
      "2 hours of my life I'm never getting back -- Neg\n",
      "So funny -- Neg\n",
      "Mind blown -- Neg\n",
      "the movie was dope -- Neg\n",
      "stupid movie -- Neg\n",
      "\n",
      "There are 4 wrong assignments\n"
     ]
    }
   ],
   "source": [
    "# new test set\n",
    "my_reviews = [\"favourite movie ever\",\n",
    "             \"2 hours of my life I'm never getting back\",\n",
    "             \"So funny\",\n",
    "             \"Mind blown\",\n",
    "             \"the movie was dope\",\n",
    "             \"stupid movie\"]\n",
    "my_tags = [\"Pos\",\"Neg\",\"Pos\",\"Pos\",\"Pos\",\"Neg\"]\n",
    "\n",
    "# test baseline polarity algorithm\n",
    "print(baselinePolarity(\"favourite movie ever!\"))\n",
    "\n",
    "nbWrong1 = 0\n",
    "for i in range(len(my_reviews)):\n",
    "    polarity = baselinePolarity(my_reviews[i])\n",
    "    print(my_reviews[i] + \" -- \" + polarity)\n",
    "    if (polarity != my_tags[i]):\n",
    "        nbWrong1 += 1\n",
    "\n",
    "print('\\nThere are %s wrong assignments' %nbWrong1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Supervised learning method\n",
    "\n",
    "We will now train a supervised learning model for polarity detection.  If you haven't done it already, you will need to install the package **sklearn**, using *pip install sklearn*.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***6.1 Training data***  \n",
    "\n",
    "In supervised learning, we need training data.  This training data must be *different* but *representative* of the eventual test data.\n",
    "\n",
    "Let's define a *training set*.  Usually a training set should be as large and varied as possible.  Training sets are very valuable, but they are costly to obtain, as they require tagging (human annotation) to generate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small training set... normally we require hundreds of sentences\n",
    "\n",
    "train_reviews = [\"this movie was stupid, so stupid\",\n",
    "                  \"I hated this movie\",\n",
    "                  \"I loved it\",\n",
    "                  \"What a waste of time\",\n",
    "                  \"Amazing!\",\n",
    "                  \"What a pity\",\n",
    "                  \"Very good movie indeed.  Glad I saw it.\"]\n",
    "\n",
    "train_tags = [\"Neg\", \"Neg\", \"Pos\", \"Neg\", \"Pos\", \"Neg\", \"Pos\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***6.2 Pre-processing of input data*** \n",
    "\n",
    "This Machine Learning package, *scikit-learn*, is somewhat particular in the way the data must be formatted to be used by the training algorithms.  So, we must perform some preprocessing on the sentences above.  Luckily *scikit-learn* provides some pre-defined functions for doing text pre-processing.  \n",
    "\n",
    "We easily transform each sentence into a list of indexes into a dictionary.  The dictionary is built from the words in the sentences.  The keys of the dictionary are the words, and the value is an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# The CountVectorizer builds a dictionary of all words (count_vect.vocabulary_), \n",
    "# and generates a matrix (train_counts), to represent each sentence\n",
    "# as a set of indices into the dictionary. The words in the dictionary are the words found in train_reviews.\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "train_counts = count_vect.fit_transform(train_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what the code above does, first let's print the vocabulary gathered from the sentences in train_reviews.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 13, 'movie': 7, 'was': 16, 'stupid': 12, 'so': 11, 'hated': 3, 'loved': 6, 'it': 5, 'what': 18, 'waste': 17, 'of': 8, 'time': 14, 'amazing': 0, 'pity': 9, 'very': 15, 'good': 2, 'indeed': 4, 'glad': 1, 'saw': 10}\n"
     ]
    }
   ],
   "source": [
    "# print the vocabulary (dictionary of words)\n",
    "print(count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can interpret the output above as: \n",
    "\n",
    "'saw':10  to mean that the word 'saw' has been assigned index 10  \n",
    "'time':14 to mean that the word 'time' has been assigned index 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's print the *train_counts*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 11)\t1\n",
      "  (0, 12)\t2\n",
      "  (0, 16)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 13)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 13)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 6)\t1\n",
      "  (3, 14)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 17)\t1\n",
      "  (3, 18)\t1\n",
      "  (4, 0)\t1\n",
      "  (5, 9)\t1\n",
      "  (5, 18)\t1\n",
      "  (6, 10)\t1\n",
      "  (6, 1)\t1\n",
      "  (6, 4)\t1\n",
      "  (6, 2)\t1\n",
      "  (6, 15)\t1\n",
      "  (6, 5)\t1\n",
      "  (6, 7)\t1\n"
     ]
    }
   ],
   "source": [
    "# print the content of the training examples in terms of frequency of words (each word represented by its index)\n",
    "print(train_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can interpret each line above as:  \n",
    "\n",
    "(0, 11) 1  -- sentence 0 (in train_reviews) has 1 time the word 11 (index of the word in count_vect.vocabulary, that is the word 'so')  \n",
    "(0, 12) 2  -- sentence 0 (in train_reviews) has 2 times word 12 (index of the word in count_vect.vocabulary, that is the word 'stupid')  \n",
    "\n",
    "So the train_counts contain for each sentence, the BOW associated with that sentence, but in the form of a list of indexes (each index corresponding to a word)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***6.3 Naive Bayes learning***\n",
    "\n",
    "With the data preprocessed, we are ready to test the Naive Bayes algorithm provided by scikit-learn.  That algorithm required the training data to be represented in terms of *train counts* which is why we did the pre-processing above.\n",
    "\n",
    "It's as easy as performing *fit*, as you see below, to train the model.  But you know what's underneath!!!  It creates prior probabilities for classes (Neg, Pos) and posterior probabilities of words (features) per class (e.g. P(stupid|Pos) or P(stupid|Neg).  All these probabilities are used in Bayes Theorem.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO-DO - Q2)** What are the prior probabilities of the Pos and Neg classes using the training set above.  (this does not require coding, just do it manually)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers  \n",
    "\n",
    "P(Pos) =  3/7    (num of docs (pos)/ total number of docs)\n",
    "P(Neg) = 4/7   (num of docs (pos)/ total number of docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of a naive bayes algorithm, the \"fit\" is the training\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Training the model\n",
    "clf = MultinomialNB().fit(train_counts, train_tags)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***6.4 Evaluation***\n",
    "\n",
    "Let's first look at how the model performs on the training set, on which it learned.  To apply the model for classification (prediction), we use the *predict* method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'this movie was stupid, so stupid' => Neg\n",
      "'I hated this movie' => Neg\n",
      "'I loved it' => Pos\n",
      "'What a waste of time' => Neg\n",
      "'Amazing!' => Pos\n",
      "'What a pity' => Neg\n",
      "'Very good movie indeed.  Glad I saw it.' => Pos\n"
     ]
    }
   ],
   "source": [
    "# Testing on training set\n",
    "predicted = clf.predict(train_counts)\n",
    "for doc, category in zip(train_reviews, predicted):   # zip allows to go through two lists simultaneously\n",
    "    print('%r => %s' % (doc, category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, on the training set we get 100%, no mistakes....  But we should test on a real **test set**.  We have two test sets, one that I created above (test_reviews) and the one that you created (my_reviews).  \n",
    "\n",
    "**(TO_DO - Q3)** Test the trained model on these two test sets.  Write the code below to do so.  Before testing, each test set must be transformed through the preprocessing steps, so their format is compatible with the learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Can't believe I wasted my time on this\" => Neg\n",
      "'Really awful' => Neg\n",
      "'Actors were good' => Pos\n",
      "'So boring' => Neg\n",
      "'Stayed at the edge of my seat' => Neg\n",
      "'I never like any movie' => Neg\n",
      "'Argghhhh I hated it' => Pos\n",
      "'favourite movie ever' => Neg\n",
      "\"2 hours of my life I'm never getting back\" => Neg\n",
      "'So funny' => Neg\n",
      "'Mind blown' => Neg\n",
      "'the movie was dope' => Neg\n",
      "'stupid movie' => Neg\n"
     ]
    }
   ],
   "source": [
    "# Pre-process test set test_reviews\n",
    "test_reviews_counts = count_vect.transform(test_reviews)\n",
    "# predict the results\n",
    "testPredicted = clf.predict(test_reviews_counts)\n",
    "# print the results\n",
    "for doc, category in zip(test_reviews, testPredicted):   # zip allows to go through two lists simultaneously\n",
    "    print('%r => %s' % (doc, category))\n",
    "\n",
    "\n",
    "# Pre-process the test set my_reviews\n",
    "my_reviews_counts = count_vect.transform(my_reviews)\n",
    "\n",
    "# predict the results\n",
    "myPredicted = clf.predict(my_reviews_counts)\n",
    "# print the results\n",
    "for doc, category in zip(my_reviews, myPredicted):   # zip allows to go through two lists simultaneously\n",
    "    print('%r => %s' % (doc, category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO_DO - Q4)** A common **Evaluation Measure**, related to *number of wrong choices* is called **Recall**.  Recall is the number of good assignments for a particular class, divided by the number of test examples of that class.  For example, if the test set contains 5 positive examples and the algorithm only found 2, then the recall for the class Pos is 2/5.  Write a small method below that will calculate a class' recall.  It will receive three parameters, (1) the set of correct tags (e.g. (Pos, Neg, Pos)), (2) the predictions (e.g (Pos, Pos, Neg)) and (3) the class of interest (e.g. Pos).  It will return the recall of that class (e.g. 50%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number wrong\n",
    "def recall(goodTags, predictions, classOfInterest):\n",
    "    count = 0\n",
    "    div = 0\n",
    "    for i in range(len(goodTags)):\n",
    "        if(goodTags[i]  == classOfInterest):\n",
    "            div += 1\n",
    "            if(goodTags[i] == predictions[i]):\n",
    "                count += 1\n",
    "    return count/div\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO - Q5)** Use the recall method to calculate the recall on the two test sets.  Print those recalls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.8\n",
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# recall\n",
    "print(recall(test_tags, testPredicted, \"Pos\"))\n",
    "print(recall(test_tags, testPredicted, \"Neg\"))\n",
    "print(recall(my_tags,myPredicted,\"Pos\"))\n",
    "print(recall(my_tags,myPredicted,\"Neg\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Discussion\n",
    "\n",
    "**(TO DO - Q6)** Is the Naive Bayes approach performing much better than the baseline approach?  Present and discuss the results below.  Give two suggestions to help the Naive Bayes approach within the context of our experiment of polarity detection for movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparative results: Baseline: 2 wrong, 4 wrong. NB: 2 wrong, 4 wrong. The results didn't improve using NB. With such a small data set it is hard for the NB method to learn. \n",
    "\n",
    "Three suggestions:  \n",
    "1) Have a bigger training and testing data set. With more data the probabilities of if its positive or negative will be more clear\n",
    "\n",
    "2) Allow a neutral option. Sometimes the reviews can have both positive and negative aspects. the reviewer can just be giving a neutral review. so if theres an equal amount of positive and negative probability it can be given as neutral.\n",
    "\n",
    "3) allow for a smoothing technique such as laplace. I put words such as \"dope\" that do not appear in teh features so it has a zero probability making the set of features equal 0. Adding 1 to all the features would avoid this. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO-DO - OPTIONAL)**  We only used two classes Positive and Negative.  But... it would be nice to also introduce a Neutral class for when the review is neither positive, nor negative... just neutral.  Perform a new experimental set-up in which you train and test the naive bayes classifier with 3 values.  You can also change the baseline classifier to predict three values.  If you do this optional task, do it below, do not change the cells above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signature\n",
    "\n",
    "I, Felix Singerman, declare that the answers provided in this notebook are my own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
